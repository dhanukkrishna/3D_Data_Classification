{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNF7_QB8_JK6",
        "outputId": "a04fa0a0-3635-4092-b82a-dfe5c524467e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6wOR0FAQ_m_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "cLV6nL3u_nCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "GsxNgehm_nE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding random seed fix"
      ],
      "metadata": {
        "id": "9vtwPxd5_nHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "fix_seed()"
      ],
      "metadata": {
        "id": "Sb1sc0LXE73J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def square_distance(src, dst):\n",
        "    B, N, _ = src.shape\n",
        "    _, M, _ = dst.shape\n",
        "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
        "    dist += torch.sum(src ** 2, -1).unsqueeze(-1)\n",
        "    dist += torch.sum(dst ** 2, -1).unsqueeze(1)\n",
        "    return dist\n",
        "\n",
        "def index_points(points, idx):\n",
        "    device = points.device\n",
        "    B = points.shape[0]\n",
        "    new_shape = list(idx.shape)\n",
        "    idx_flat = idx.reshape(B, -1)\n",
        "    batch_indices = torch.arange(B, device=device).view(B, 1).repeat(1, idx_flat.shape[1])\n",
        "    result = points[batch_indices, idx_flat]\n",
        "    return result.reshape(*new_shape, points.shape[-1])\n",
        "\n",
        "def farthest_point_sample(xyz, npoint):\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    centroids = torch.zeros(B, npoint, dtype=torch.long, device=device)\n",
        "    distance  = torch.ones(B, N, device=device) * 1e10\n",
        "    farthest = torch.randint(0, N, (B,), device=device)\n",
        "    batch_indices = torch.arange(B, device=device)\n",
        "\n",
        "    for i in range(npoint):\n",
        "        centroids[:, i] = farthest\n",
        "        centroid = xyz[batch_indices, farthest].unsqueeze(1)\n",
        "        dist = torch.sum((xyz - centroid)**2, -1)\n",
        "        mask = dist < distance\n",
        "        distance[mask] = dist[mask]\n",
        "        farthest = torch.max(distance, -1)[1]\n",
        "\n",
        "    return centroids\n",
        "\n",
        "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
        "    device = xyz.device\n",
        "    B, N, C = xyz.shape\n",
        "    _, S, _ = new_xyz.shape\n",
        "    sqrdists = square_distance(new_xyz, xyz)\n",
        "\n",
        "    group_idx = torch.arange(N, device=device).view(1,1,N).repeat(B,S,1)\n",
        "    group_idx[sqrdists > radius**2] = N\n",
        "\n",
        "    group_idx = group_idx.sort(dim=-1)[0][:,:,:nsample]\n",
        "\n",
        "    first = group_idx[:,:,0].unsqueeze(-1).repeat(1,1,nsample)\n",
        "    mask = group_idx == N\n",
        "    group_idx[mask] = first[mask]\n",
        "\n",
        "    return group_idx\n",
        "\n",
        "def sample_and_group(npoint, radius, nsample, xyz, points):\n",
        "    B, N, C = xyz.shape\n",
        "    fps_idx = farthest_point_sample(xyz, npoint)\n",
        "    new_xyz = index_points(xyz, fps_idx)\n",
        "    idx = query_ball_point(radius, nsample, xyz, new_xyz)\n",
        "\n",
        "    grouped_xyz = index_points(xyz, idx)\n",
        "    grouped_xyz_norm = grouped_xyz - new_xyz.unsqueeze(2)\n",
        "\n",
        "    if points is not None:\n",
        "        grouped_points = index_points(points, idx)\n",
        "        new_points = torch.cat([grouped_xyz_norm, grouped_points], dim=-1)\n",
        "    else:\n",
        "        new_points = grouped_xyz_norm\n",
        "\n",
        "    return new_xyz, new_points\n",
        "\n",
        "def sample_and_group_all(xyz, points):\n",
        "    B, N, C = xyz.shape\n",
        "    new_xyz = torch.zeros(B,1,C).to(xyz.device)\n",
        "    new_points = xyz.view(B,1,N,C)\n",
        "    if points is not None:\n",
        "        new_points = torch.cat([new_points, points.view(B,1,N,-1)], dim=-1)\n",
        "    return new_xyz, new_points"
      ],
      "metadata": {
        "id": "Rej9B38Bz-7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xb8PRDCJ_nK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNetSetAbstraction(nn.Module):\n",
        "    def __init__(self, npoint, radius, nsample, in_channel, mlp, group_all):\n",
        "        super().__init__()\n",
        "        self.npoint = npoint\n",
        "        self.radius = radius\n",
        "        self.nsample = nsample\n",
        "        self.group_all = group_all\n",
        "        self.mlp_convs = nn.ModuleList()\n",
        "        self.mlp_bns   = nn.ModuleList()\n",
        "\n",
        "        last = in_channel\n",
        "        for out in mlp:\n",
        "            self.mlp_convs.append(nn.Conv2d(last, out, 1))\n",
        "            self.mlp_bns.append(nn.BatchNorm2d(out))\n",
        "            last = out\n",
        "\n",
        "    def forward(self, xyz, points):\n",
        "        xyz = xyz.permute(0,2,1)\n",
        "        if points is not None:\n",
        "            points = points.permute(0,2,1)\n",
        "\n",
        "        if self.group_all:\n",
        "            new_xyz, new_points = sample_and_group_all(xyz, points)\n",
        "        else:\n",
        "            new_xyz, new_points = sample_and_group(self.npoint, self.radius, self.nsample, xyz, points)\n",
        "\n",
        "        new_points = new_points.permute(0,3,2,1)\n",
        "        for conv, bn in zip(self.mlp_convs, self.mlp_bns):\n",
        "            new_points = F.relu(bn(conv(new_points)))\n",
        "\n",
        "        new_points = torch.max(new_points,2)[0]\n",
        "        new_xyz = new_xyz.permute(0,2,1)\n",
        "        return new_xyz, new_points\n",
        "\n",
        "class PointNetSetAbstractionMsg(nn.Module):\n",
        "    def __init__(self, npoint, radius_list, nsample_list, in_channel, mlp_list):\n",
        "        super().__init__()\n",
        "        self.npoint = npoint\n",
        "        self.radius_list = radius_list\n",
        "        self.nsample_list = nsample_list\n",
        "\n",
        "        self.conv_blocks = nn.ModuleList()\n",
        "        self.bn_blocks   = nn.ModuleList()\n",
        "\n",
        "        for mlp in mlp_list:\n",
        "            convs = nn.ModuleList()\n",
        "            bns   = nn.ModuleList()\n",
        "            last = in_channel + 3\n",
        "            for out in mlp:\n",
        "                convs.append(nn.Conv2d(last, out, 1))\n",
        "                bns.append(nn.BatchNorm2d(out))\n",
        "                last = out\n",
        "            self.conv_blocks.append(convs)\n",
        "            self.bn_blocks.append(bns)\n",
        "\n",
        "    def forward(self, xyz, points):\n",
        "        xyz = xyz.permute(0,2,1)\n",
        "        if points is not None:\n",
        "            points = points.permute(0,2,1)\n",
        "\n",
        "        B,N,C = xyz.shape\n",
        "        S = self.npoint\n",
        "        new_xyz = index_points(xyz, farthest_point_sample(xyz,S))\n",
        "\n",
        "        new_points_list = []\n",
        "        for i,radius in enumerate(self.radius_list):\n",
        "            K = self.nsample_list[i]\n",
        "            idx = query_ball_point(radius, K, xyz, new_xyz)\n",
        "            grouped_xyz = index_points(xyz, idx)\n",
        "            grouped_xyz -= new_xyz.unsqueeze(2)\n",
        "\n",
        "            if points is not None:\n",
        "                grouped_points = index_points(points, idx)\n",
        "                grouped_points = torch.cat([grouped_points, grouped_xyz], -1)\n",
        "            else:\n",
        "                grouped_points = grouped_xyz\n",
        "\n",
        "            grouped_points = grouped_points.permute(0,3,2,1)\n",
        "\n",
        "            for conv, bn in zip(self.conv_blocks[i], self.bn_blocks[i]):\n",
        "                grouped_points = F.relu(bn(conv(grouped_points)))\n",
        "\n",
        "            # Corrected line: apply max pooling to grouped_points\n",
        "            new_points_agg = torch.max(grouped_points,2)[0]\n",
        "            new_points_list.append(new_points_agg)\n",
        "\n",
        "        new_xyz = new_xyz.permute(0,2,1)\n",
        "        return new_xyz, torch.cat(new_points_list, 1)"
      ],
      "metadata": {
        "id": "LQqvICQVz--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3qOB7U4p_nNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class get_model(nn.Module):\n",
        "    def __init__(self, num_class, normal_channel=True):\n",
        "        super().__init__()\n",
        "        in_channel = 3 if normal_channel else 0\n",
        "        self.normal_channel = normal_channel\n",
        "\n",
        "        self.sa1 = PointNetSetAbstractionMsg(\n",
        "            512, [0.1,0.2,0.4], [16,32,128], in_channel,\n",
        "            [[32,32,64],[64,64,128],[64,96,128]]\n",
        "        )\n",
        "        self.sa2 = PointNetSetAbstractionMsg(\n",
        "            128, [0.2,0.4,0.8], [32,64,128], 320,\n",
        "            [[64,64,128],[128,128,256],[128,128,256]]\n",
        "        )\n",
        "        self.sa3 = PointNetSetAbstraction(\n",
        "            None, None, None, 640+3, [256,512,1024], True\n",
        "        )\n",
        "\n",
        "        self.fc1 = nn.Linear(1024,512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.drop1 = nn.Dropout(0.4)\n",
        "\n",
        "        self.fc2 = nn.Linear(512,256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc3 = nn.Linear(256,num_class)\n",
        "        self.drop3 = nn.Dropout(0.6)   #Adding extra Dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "\n",
        "        if self.normal_channel:\n",
        "            norm = x[:,3:,:]\n",
        "            xyz  = x[:,:3,:]\n",
        "        else:\n",
        "            norm = None\n",
        "            xyz  = x\n",
        "\n",
        "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
        "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
        "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
        "\n",
        "        x = l3_points.view(B,1024)\n",
        "        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
        "        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x,-1), l3_points\n",
        "\n",
        "class get_loss(nn.Module):\n",
        "    def forward(self, pred, target, _=None):\n",
        "        return F.nll_loss(pred, target)"
      ],
      "metadata": {
        "id": "zRLxlhE4z_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9bjm1T6__nQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pc_normalize(pc):\n",
        "    centroid = pc.mean(axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.linalg.norm(pc, axis=1))\n",
        "    return pc / m\n",
        "\n",
        "class ModelNetNPY_Split(Dataset):\n",
        "    def __init__(self, file_list, labels, num_points=1024, use_normals=True):\n",
        "        self.files = file_list\n",
        "        self.labels = labels\n",
        "        self.num_points = num_points\n",
        "        self.use_normals = use_normals\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pc = np.load(self.files[idx]).astype(np.float32)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        pc[:, :3] = pc_normalize(pc[:, :3])\n",
        "\n",
        "        if pc.shape[0] >= self.num_points:\n",
        "            choice = np.random.choice(pc.shape[0], self.num_points, replace=False)\n",
        "        else:\n",
        "            choice = np.random.choice(pc.shape[0], self.num_points, replace=True)\n",
        "\n",
        "        pc = pc[choice]\n",
        "\n",
        "        if not self.use_normals:\n",
        "            pc = pc[:, :3]\n",
        "\n",
        "        pc = torch.from_numpy(pc).float().transpose(0, 1)\n",
        "        label = torch.tensor(label).long()\n",
        "\n",
        "        return pc, label"
      ],
      "metadata": {
        "id": "ImsXfprJz_FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35bd0Bj0_nUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_splits(root, ratio=0.8):\n",
        "    classes = sorted(os.listdir(root))\n",
        "    class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "\n",
        "    train_f = []\n",
        "    val_f = []\n",
        "    test_f = []\n",
        "\n",
        "    train_l = []\n",
        "    val_l = []\n",
        "    test_l = []\n",
        "\n",
        "    for cls in classes:\n",
        "        label = class_to_idx[cls]\n",
        "\n",
        "        train_dir = os.path.join(root, cls, \"train\")\n",
        "        test_dir = os.path.join(root, cls, \"test\")\n",
        "\n",
        "        # gather files\n",
        "        train_list = [os.path.join(train_dir, f) for f in os.listdir(train_dir)\n",
        "                      if f.endswith('.npy')]\n",
        "        random.shuffle(train_list)\n",
        "\n",
        "        split_idx = int(len(train_list) * ratio)\n",
        "\n",
        "        # train\n",
        "        train_f += train_list[:split_idx]\n",
        "        train_l += [label] * split_idx\n",
        "\n",
        "        # val\n",
        "        val_f += train_list[split_idx:]\n",
        "        val_l += [label] * (len(train_list) - split_idx)\n",
        "\n",
        "        # test\n",
        "        test_list = [os.path.join(test_dir, f) for f in os.listdir(test_dir)\n",
        "                     if f.endswith('.npy')]\n",
        "        test_f += test_list\n",
        "        test_l += [label] * len(test_list)\n",
        "\n",
        "    return (train_f, train_l), (val_f, val_l), (test_f, test_l)"
      ],
      "metadata": {
        "id": "ePWJNkeNz_IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = \"/content/drive/MyDrive/Colab Notebooks/ModelNet10_normals\""
      ],
      "metadata": {
        "id": "yBI4UMz1_nXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9-G9aEO4_naB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_f, train_l), (val_f, val_l), (test_f, test_l) = create_splits(ROOT)"
      ],
      "metadata": {
        "id": "aijArBFwz_PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70VZY6d2_ncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ModelNetNPY_Split(train_f, train_l, use_normals=False)\n",
        "val_ds   = ModelNetNPY_Split(val_f, val_l, use_normals=False)\n",
        "test_ds  = ModelNetNPY_Split(test_f, test_l, use_normals=False)"
      ],
      "metadata": {
        "id": "QrInp3r6z_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQLMJ1R8_nfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "WoQiwlUuz_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCnWkva1_nia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "cYKudmlVz_ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VcvUnMVT_nma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(os.listdir(ROOT))\n",
        "model = get_model(num_classes, normal_channel=False).to(DEVICE)\n",
        "\n",
        "criterion = get_loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#Adding learning Rate Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=10,     # reduce LR every 10 epochs\n",
        "    gamma=0.5         # multiply LR by 0.5\n",
        ")\n",
        "\n",
        "\n",
        "best_val_acc = 0\n",
        "history = []"
      ],
      "metadata": {
        "id": "-UACOuD81EaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ePa-0Tv_np5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Device:\", DEVICE if DEVICE == \"cuda\" else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf-TcxOh1Ed-",
        "outputId": "4d763711-763a-494d-fc58-0d2cfcad2b26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 10\n",
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "root = \"/content/drive/MyDrive/Colab Notebooks/ModelNet10_normals\"  # change\n",
        "\n",
        "bad_files = []\n",
        "\n",
        "for cls in os.listdir(root):\n",
        "    class_path = os.path.join(root, cls)\n",
        "    for split in [\"train\", \"test\", \"val\"]:\n",
        "        split_path = os.path.join(class_path, split)\n",
        "        if not os.path.exists(split_path):\n",
        "            continue\n",
        "        for f in os.listdir(split_path):\n",
        "            fp = os.path.join(split_path, f)\n",
        "\n",
        "            try:\n",
        "                arr = np.load(fp)\n",
        "                if arr.size == 0 or arr.ndim == 0:\n",
        "                    print(\"Empty array:\", fp)\n",
        "                    bad_files.append(fp)\n",
        "            except Exception as e:\n",
        "                print(\"Corrupted file:\", fp, \"| Error:\", e)\n",
        "                bad_files.append(fp)\n",
        "\n",
        "print(\"\\nTotal corrupted files:\", len(bad_files))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8UadvNp_nsp",
        "outputId": "eadc080a-53d1-4bf5-f951-5df0b62e187b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total corrupted files: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for f in bad_files:\n",
        "    os.remove(f)\n",
        "    print(\"Deleted:\", f)"
      ],
      "metadata": {
        "id": "0lGnVSAuNRsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "cSg9Jyao5Gtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "best_val_acc = 0\n",
        "history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for pc, labels in tqdm(train_loader):\n",
        "        pc, labels = pc.to(DEVICE), labels.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred, _ = model(pc)\n",
        "        loss = criterion(pred, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_correct += (pred.argmax(1) == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for pc, labels in val_loader:\n",
        "            pc, labels = pc.to(DEVICE), labels.to(DEVICE)\n",
        "            pred, _ = model(pc)\n",
        "            loss = criterion(pred, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = pred.argmax(1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # Calculate precision, recall, f1\n",
        "    precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} | Train Acc={train_acc:.4f} | Val Acc={val_acc:.4f} | \"\n",
        "        f\"P={precision:.4f} | R={recall:.4f} | F1={f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/model_dec_11.pth\")\n",
        "        best_val_acc = val_acc\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save history\n",
        "    history.append({\n",
        "        \"epoch\": epoch+1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_acc\": val_acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDBkTeNz5veN",
        "outputId": "e7a8a5c0-ce32-4122-a700-0f9ce0c0615c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 2/200 [01:20<2:11:02, 39.71s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6CJIVnZ5wIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 20\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     model.train()\n",
        "#     train_loss = 0\n",
        "#     train_correct = 0\n",
        "#     train_total = 0\n",
        "\n",
        "#     for pc, labels in tqdm(train_loader):\n",
        "#         pc, labels = pc.to(DEVICE), labels.to(DEVICE)\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         pred, _ = model(pc)\n",
        "#         loss = criterion(pred, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_loss += loss.item()\n",
        "#         train_correct += (pred.argmax(1) == labels).sum().item()\n",
        "#         train_total += labels.size(0)\n",
        "\n",
        "#     train_acc = train_correct / train_total\n",
        "\n",
        "#     # ---------- Validation ----------\n",
        "#     model.eval()\n",
        "#     val_correct = 0\n",
        "#     val_total = 0\n",
        "#     val_loss = 0\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for pc, labels in val_loader:\n",
        "#             pc, labels = pc.to(DEVICE), labels.to(DEVICE)\n",
        "#             pred, _ = model(pc)\n",
        "#             loss = criterion(pred, labels)\n",
        "#             val_loss += loss.item()\n",
        "#             val_correct += (pred.argmax(1) == labels).sum().item()\n",
        "#             val_total += labels.size(0)\n",
        "\n",
        "#     val_acc = val_correct / val_total\n",
        "\n",
        "#     print(f\"Epoch {epoch+1} | Train Acc={train_acc:.4f} | Val Acc={val_acc:.4f}\")\n",
        "\n",
        "#     # Save best model\n",
        "#     if val_acc > best_val_acc:\n",
        "#         torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/model_dec_11.pth\")\n",
        "#         best_val_acc = val_acc\n",
        "#         print(\"Saved best model\")\n",
        "\n",
        "#     # Update learning rate\n",
        "#     scheduler.step()\n",
        "\n",
        "#     # Save history\n",
        "#     history.append({\n",
        "#         \"epoch\": epoch+1,\n",
        "#         \"train_loss\": train_loss,\n",
        "#         \"train_acc\": train_acc,\n",
        "#         \"val_loss\": val_loss,\n",
        "#         \"val_acc\": val_acc\n",
        "#     })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFc5969QCpk3",
        "outputId": "51aab4eb-91f9-4698-d48e-b070d0ec306d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Acc=0.9144 | Val Acc=0.9075\n",
            "Saved best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Acc=0.9204 | Val Acc=0.8775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Acc=0.9282 | Val Acc=0.8988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Acc=0.9414 | Val Acc=0.9537\n",
            "Saved best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Acc=0.9420 | Val Acc=0.9250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Acc=0.9439 | Val Acc=0.9450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Acc=0.9508 | Val Acc=0.9437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Acc=0.9486 | Val Acc=0.9563\n",
            "Saved best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Train Acc=0.9505 | Val Acc=0.9413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Acc=0.9564 | Val Acc=0.9363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Acc=0.9640 | Val Acc=0.9663\n",
            "Saved best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Acc=0.9730 | Val Acc=0.9675\n",
            "Saved best model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Acc=0.9734 | Val Acc=0.9637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Acc=0.9771 | Val Acc=0.9613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Acc=0.9724 | Val Acc=0.9675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Acc=0.9759 | Val Acc=0.9613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Acc=0.9809 | Val Acc=0.9613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Acc=0.9712 | Val Acc=0.9637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:17<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Acc=0.9762 | Val Acc=0.9563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [02:16<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Acc=0.9715 | Val Acc=0.9600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETa9kX10_nvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(history)\n",
        "df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/train_metrics_Dec_11.csv\", index=False)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "collapsed": true,
        "id": "qPaHWUft1Eko",
        "outputId": "f9e8fec1-5938-4ea4-e33f-5fe4338c765b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-304312875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/train_metrics_Dec_11.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBsO3qEi_nya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "uGFQ-Do21Enm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8RkNutkAVkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab Notebooks/model_dec_11.pth\"))\n",
        "model.to(DEVICE)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GKAkdgVw1Equ",
        "outputId": "7d2cb95a-f52e-4744-cb71-c9cfc340170c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "get_model(\n",
              "  (sa1): PointNetSetAbstractionMsg(\n",
              "    (conv_blocks): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (bn_blocks): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0-1): 2 x BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): ModuleList(\n",
              "        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): ModuleList(\n",
              "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (sa2): PointNetSetAbstractionMsg(\n",
              "    (conv_blocks): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0): Conv2d(323, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (1-2): 2 x ModuleList(\n",
              "        (0): Conv2d(323, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (bn_blocks): ModuleList(\n",
              "      (0): ModuleList(\n",
              "        (0-1): 2 x BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1-2): 2 x ModuleList(\n",
              "        (0-1): 2 x BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (sa3): PointNetSetAbstraction(\n",
              "    (mlp_convs): ModuleList(\n",
              "      (0): Conv2d(643, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (mlp_bns): ModuleList(\n",
              "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (drop1): Dropout(p=0.4, inplace=False)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (drop2): Dropout(p=0.5, inplace=False)\n",
              "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LzFm8shAVna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "BgW8bGZT1EuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7lR-GJgAVtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for pc, labels in test_loader:\n",
        "        pc, labels = pc.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        outputs, _ = model(pc)\n",
        "        predictions = outputs.argmax(1)\n",
        "\n",
        "        # LOSS\n",
        "        loss = criterion(outputs, labels).item()\n",
        "\n",
        "        # Convert to CPU numpy\n",
        "        y_true = labels.cpu().numpy()\n",
        "        y_pred = predictions.cpu().numpy()\n",
        "\n",
        "        # METRICS\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "        # SAVE ROW\n",
        "        results.append({\n",
        "            \"Loss\": loss,\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision\": prec,\n",
        "            \"Recall\": rec,\n",
        "            \"F1\": f1\n",
        "        })\n",
        "\n",
        "# CONVERT TO CSV\n",
        "df = pd.DataFrame(results)\n",
        "csv_path = \"/content/drive/MyDrive/Colab Notebooks/test_metrics_Dec_11.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"Saved CSV at:\", csv_path)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "collapsed": true,
        "id": "kzGx6-NH1ExC",
        "outputId": "a4aee4de-adac-4f48-f053-8b01331ac082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved CSV at: /content/drive/MyDrive/Colab Notebooks/test_metrics_normal.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Loss  Accuracy  Precision    Recall        F1\n",
              "0  0.361698    0.9375   0.500000  0.468750  0.483871\n",
              "1  0.925782    0.8125   0.333333  0.270833  0.298851\n",
              "2  0.432742    0.8750   0.500000  0.437500  0.466667\n",
              "3  0.007620    1.0000   1.000000  1.000000  1.000000\n",
              "4  0.000300    1.0000   1.000000  1.000000  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcebd6fd-6b6a-4d7b-ad01-c33cea660baa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.361698</td>\n",
              "      <td>0.9375</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.468750</td>\n",
              "      <td>0.483871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.925782</td>\n",
              "      <td>0.8125</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.270833</td>\n",
              "      <td>0.298851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.432742</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.466667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.007620</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000300</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcebd6fd-6b6a-4d7b-ad01-c33cea660baa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dcebd6fd-6b6a-4d7b-ad01-c33cea660baa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dcebd6fd-6b6a-4d7b-ad01-c33cea660baa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9db5f5db-fa6c-4c12-b95d-a0ce27610da6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9db5f5db-fa6c-4c12-b95d-a0ce27610da6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9db5f5db-fa6c-4c12-b95d-a0ce27610da6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 57,\n  \"fields\": [\n    {\n      \"column\": \"Loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27719203796106023,\n        \"min\": 8.851144229993224e-06,\n        \"max\": 1.0968657732009888,\n        \"num_unique_values\": 57,\n        \"samples\": [\n          0.3616977632045746,\n          0.00032116766669787467,\n          3.0367407816811465e-05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11620239141548122,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.9375,\n          0.8125,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27480658244572637,\n        \"min\": 0.3333333333333333,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3333333333333333,\n          0.6666666666666666,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.310890314076492,\n        \"min\": 0.16666666666666666,\n        \"max\": 1.0,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.40625,\n          0.25,\n          0.46875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2957791126450834,\n        \"min\": 0.2222222222222222,\n        \"max\": 1.0,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.2857142857142857,\n          0.4482758620689655,\n          0.4838709677419355\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Ms-KWm3AVww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mr-JxotAAVzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0EqKJT5AV2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-4cEQ74AV5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_HQQTJfOAV9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9j_pvP5OAWBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aGjCXAPAWEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D66h7Kp0AWGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EjfutEODAWJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1DfTUwLpAWMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0b-LUWQAWP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUVHI4qt_n1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}